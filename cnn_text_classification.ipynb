{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Text Classification\n",
    "\n",
    "This notebook demonstrates how to build a Convolutional Neural Network (CNN) for text classification tasks.\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction to CNN for Text\n",
    "2. Data Loading and Preprocessing\n",
    "3. Text Vectorization\n",
    "4. Building the CNN Model\n",
    "5. Training and Evaluation\n",
    "6. Predictions and Analysis\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand how CNNs can be applied to text data\n",
    "- Learn about text preprocessing and embedding\n",
    "- Build and train a CNN model for text classification\n",
    "- Evaluate model performance and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to CNN for Text Classification\n",
    "\n",
    "### Why CNNs for Text?\n",
    "While CNNs are primarily known for image processing, they work remarkably well for text classification:\n",
    "- **Local feature detection**: CNNs can identify n-gram patterns in text\n",
    "- **Parameter efficiency**: Fewer parameters than RNNs for similar tasks\n",
    "- **Parallel processing**: Faster training compared to sequential models\n",
    "- **Translation invariance**: Can detect features regardless of position in text\n",
    "\n",
    "### How it works:\n",
    "1. Text is converted to word embeddings (vectors)\n",
    "2. 1D convolution filters slide over the text sequence\n",
    "3. Filters detect local patterns (like n-grams)\n",
    "4. Pooling layers extract the most important features\n",
    "5. Dense layers classify based on extracted features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Embedding, Conv1D, GlobalMaxPooling1D,\n",
    "    MaxPooling1D, Flatten, Input, Concatenate\n",
    ")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# For text processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Display versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Exploration\n",
    "\n",
    "For this tutorial, we'll use the IMDB movie reviews dataset for sentiment analysis (binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset from Keras\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Parameters\n",
    "MAX_FEATURES = 10000  # Number of words to consider as features\n",
    "MAX_LEN = 500  # Maximum length of sequences\n",
    "\n",
    "print(\"Loading IMDB dataset...\")\n",
    "(X_train_raw, y_train), (X_test_raw, y_test) = imdb.load_data(num_words=MAX_FEATURES)\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train_raw)}\")\n",
    "print(f\"Testing samples: {len(X_test_raw)}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(f\"Positive: {sum(y_train)} ({sum(y_train)/len(y_train)*100:.2f}%)\")\n",
    "print(f\"Negative: {len(y_train) - sum(y_train)} ({(len(y_train) - sum(y_train))/len(y_train)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore sequence lengths\n",
    "train_lengths = [len(x) for x in X_train_raw]\n",
    "test_lengths = [len(x) for x in X_test_raw]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_lengths, bins=50, alpha=0.7, color='blue')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Training Sequence Lengths')\n",
    "plt.axvline(x=MAX_LEN, color='red', linestyle='--', label=f'Max Length: {MAX_LEN}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot([train_lengths, test_lengths], labels=['Train', 'Test'])\n",
    "plt.ylabel('Sequence Length')\n",
    "plt.title('Sequence Length Distribution')\n",
    "plt.axhline(y=MAX_LEN, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSequence length statistics (Training):\")\n",
    "print(f\"Mean: {np.mean(train_lengths):.2f}\")\n",
    "print(f\"Median: {np.median(train_lengths):.2f}\")\n",
    "print(f\"Max: {np.max(train_lengths)}\")\n",
    "print(f\"Min: {np.min(train_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Pad sequences to ensure uniform input size for the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "print(\"Padding sequences...\")\n",
    "X_train = pad_sequences(X_train_raw, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test_raw, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "\n",
    "# Create validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"\\nAfter validation split:\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the CNN Model\n",
    "\n",
    "### Architecture Overview:\n",
    "1. **Embedding Layer**: Converts word indices to dense vectors\n",
    "2. **Convolutional Layers**: Extract local features using different filter sizes\n",
    "3. **Pooling Layers**: Reduce dimensionality and extract most important features\n",
    "4. **Dense Layers**: Classification layers\n",
    "5. **Dropout**: Regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "EMBEDDING_DIM = 128\n",
    "NUM_FILTERS = 128\n",
    "FILTER_SIZES = [3, 4, 5]  # Different n-gram sizes\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "def build_simple_cnn(vocab_size, embedding_dim, max_length, num_filters, filter_size, dropout_rate):\n",
    "    \"\"\"\n",
    "    Build a simple CNN model for text classification.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(input_dim=vocab_size, \n",
    "                 output_dim=embedding_dim, \n",
    "                 input_length=max_length,\n",
    "                 name='embedding'),\n",
    "        \n",
    "        # Dropout for embedding\n",
    "        Dropout(dropout_rate, name='embedding_dropout'),\n",
    "        \n",
    "        # Convolutional layer\n",
    "        Conv1D(filters=num_filters, \n",
    "               kernel_size=filter_size,\n",
    "               activation='relu',\n",
    "               name='conv1d'),\n",
    "        \n",
    "        # Global max pooling\n",
    "        GlobalMaxPooling1D(name='global_max_pooling'),\n",
    "        \n",
    "        # Dense layer\n",
    "        Dense(128, activation='relu', name='dense1'),\n",
    "        Dropout(dropout_rate, name='dropout1'),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid', name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_simple_cnn(\n",
    "    vocab_size=MAX_FEATURES,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_length=MAX_LEN,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    filter_size=3,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"Simple CNN Model Architecture:\")\n",
    "print(\"=\"*50)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Filter CNN (Kim 2014 Architecture)\n",
    "\n",
    "This architecture uses multiple filter sizes to capture different n-gram patterns simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_filter_cnn(vocab_size, embedding_dim, max_length, num_filters, filter_sizes, dropout_rate):\n",
    "    \"\"\"\n",
    "    Build a multi-filter CNN model (Kim 2014 architecture).\n",
    "    Uses multiple filter sizes to capture different n-gram patterns.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(max_length,), name='input')\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding = Embedding(input_dim=vocab_size,\n",
    "                         output_dim=embedding_dim,\n",
    "                         input_length=max_length,\n",
    "                         name='embedding')(inputs)\n",
    "    \n",
    "    embedding = Dropout(dropout_rate, name='embedding_dropout')(embedding)\n",
    "    \n",
    "    # Create multiple convolutional branches\n",
    "    conv_blocks = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        conv = Conv1D(filters=num_filters,\n",
    "                     kernel_size=filter_size,\n",
    "                     activation='relu',\n",
    "                     name=f'conv_{filter_size}gram')(embedding)\n",
    "        conv = GlobalMaxPooling1D(name=f'maxpool_{filter_size}gram')(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    \n",
    "    # Concatenate all features\n",
    "    if len(conv_blocks) > 1:\n",
    "        merged = Concatenate(name='concatenate')(conv_blocks)\n",
    "    else:\n",
    "        merged = conv_blocks[0]\n",
    "    \n",
    "    # Dense layers\n",
    "    dense = Dense(128, activation='relu', name='dense1')(merged)\n",
    "    dense = Dropout(dropout_rate, name='dropout1')(dense)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation='sigmoid', name='output')(dense)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='MultiFilterCNN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build multi-filter model\n",
    "multi_model = build_multi_filter_cnn(\n",
    "    vocab_size=MAX_FEATURES,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_length=MAX_LEN,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    filter_sizes=FILTER_SIZES,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "multi_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nMulti-Filter CNN Model Architecture:\")\n",
    "print(\"=\"*50)\n",
    "multi_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the Model\n",
    "\n",
    "We'll use callbacks for early stopping and model checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training Multi-Filter CNN Model...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train the model\n",
    "history = multi_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation accuracy and loss.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0].plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('Model Accuracy Over Epochs')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[1].plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('Model Loss Over Epochs')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print best metrics\n",
    "    best_epoch = np.argmax(history.history['val_accuracy'])\n",
    "    print(f\"\\nBest Validation Accuracy: {history.history['val_accuracy'][best_epoch]:.4f} at Epoch {best_epoch + 1}\")\n",
    "    print(f\"Training Accuracy at Best Epoch: {history.history['accuracy'][best_epoch]:.4f}\")\n",
    "    print(f\"Validation Loss at Best Epoch: {history.history['val_loss'][best_epoch]:.4f}\")\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_loss, test_accuracy = multi_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_proba = multi_model.predict(X_test, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prediction Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_pred_proba[y_test == 0], bins=50, alpha=0.7, label='Actual Negative', color='red')\n",
    "plt.hist(y_pred_proba[y_test == 1], bins=50, alpha=0.7, label='Actual Positive', color='green')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Probabilities')\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "confidence = np.abs(y_pred_proba.flatten() - 0.5)\n",
    "plt.hist(confidence, bins=50, alpha=0.7, color='blue')\n",
    "plt.xlabel('Prediction Confidence (|prob - 0.5|)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Model Confidence Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage prediction confidence: {np.mean(confidence):.4f}\")\n",
    "print(f\"Predictions with confidence > 0.4: {np.sum(confidence > 0.4) / len(confidence) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Making Predictions on Custom Text\n",
    "\n",
    "Let's create a function to preprocess and predict sentiment on custom text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word index from IMDB dataset\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "def preprocess_text(text, word_index, max_len):\n",
    "    \"\"\"\n",
    "    Preprocess custom text for prediction.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and split\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Convert words to indices (add 3 to account for special tokens in IMDB dataset)\n",
    "    sequence = [word_index.get(word, 2) + 3 for word in words]  # 2 is the index for unknown words\n",
    "    \n",
    "    # Filter out words not in vocabulary\n",
    "    sequence = [idx for idx in sequence if idx < MAX_FEATURES]\n",
    "    \n",
    "    # Pad sequence\n",
    "    padded = pad_sequences([sequence], maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    return padded\n",
    "\n",
    "def predict_sentiment(text, model, word_index, max_len):\n",
    "    \"\"\"\n",
    "    Predict sentiment of custom text.\n",
    "    \"\"\"\n",
    "    # Preprocess text\n",
    "    processed = preprocess_text(text, word_index, max_len)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(processed, verbose=0)[0][0]\n",
    "    \n",
    "    # Determine sentiment\n",
    "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
    "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "    \n",
    "    return sentiment, confidence, prediction\n",
    "\n",
    "# Test with custom reviews\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout.\",\n",
    "    \"Terrible film. Waste of time and money. The worst movie I've ever seen.\",\n",
    "    \"It was okay, nothing special. Some parts were good but overall mediocre.\",\n",
    "    \"Brilliant masterpiece! A must-watch for everyone. Outstanding performances all around.\",\n",
    "    \"Boring and predictable. I fell asleep halfway through.\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Predictions for Custom Reviews:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    sentiment, confidence, raw_pred = predict_sentiment(review, multi_model, word_index, MAX_LEN)\n",
    "    print(f\"\\nReview {i}:\")\n",
    "    print(f\"Text: {review[:80]}...\" if len(review) > 80 else f\"Text: {review}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    print(f\"Confidence: {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "    print(f\"Raw Prediction Score: {raw_pred:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Interpretation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze some misclassified examples\n",
    "misclassified_idx = np.where(y_pred != y_test)[0]\n",
    "\n",
    "print(f\"Total misclassified samples: {len(misclassified_idx)}\")\n",
    "print(f\"Misclassification rate: {len(misclassified_idx) / len(y_test) * 100:.2f}%\")\n",
    "\n",
    "# Function to decode review\n",
    "def decode_review(encoded_review):\n",
    "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review if i > 0])\n",
    "\n",
    "# Show some misclassified examples\n",
    "print(\"\\nExamples of Misclassified Reviews:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(5, len(misclassified_idx))):\n",
    "    idx = misclassified_idx[i]\n",
    "    decoded = decode_review(X_test[idx])\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Review: {decoded[:200]}...\" if len(decoded) > 200 else f\"Review: {decoded}\")\n",
    "    print(f\"True Label: {'Positive' if y_test[idx] == 1 else 'Negative'}\")\n",
    "    print(f\"Predicted: {'Positive' if y_pred[idx] == 1 else 'Negative'}\")\n",
    "    print(f\"Prediction Probability: {y_pred_proba[idx][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualizing Filter Activations (Optional Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that outputs intermediate layer activations\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Get the embedding and first conv layer outputs\n",
    "layer_outputs = [layer.output for layer in multi_model.layers[1:5]]  # embedding + conv layers\n",
    "activation_model = Model(inputs=multi_model.input, outputs=layer_outputs)\n",
    "\n",
    "# Get activations for a sample\n",
    "sample_idx = 0\n",
    "sample = X_test[sample_idx:sample_idx+1]\n",
    "activations = activation_model.predict(sample, verbose=0)\n",
    "\n",
    "print(\"Activation shapes:\")\n",
    "for i, activation in enumerate(activations):\n",
    "    print(f\"Layer {i+1}: {activation.shape}\")\n",
    "\n",
    "# Visualize embedding\n",
    "embedding_output = activations[0][0]  # Shape: (max_len, embedding_dim)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.imshow(embedding_output.T, aspect='auto', cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Sequence Position')\n",
    "plt.ylabel('Embedding Dimension')\n",
    "plt.title('Word Embedding Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Comparison: Simple vs Multi-Filter CNN\n",
    "\n",
    "Let's train the simple CNN and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train simple CNN for comparison\n",
    "print(\"Training Simple CNN for comparison...\")\n",
    "\n",
    "simple_history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate simple model\n",
    "simple_test_loss, simple_test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nSimple CNN:\")\n",
    "print(f\"  Test Accuracy: {simple_test_accuracy:.4f}\")\n",
    "print(f\"  Test Loss: {simple_test_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nMulti-Filter CNN:\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nImprovement: {(test_accuracy - simple_test_accuracy)*100:.2f}% accuracy gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = 'cnn_text_classifier.keras'\n",
    "multi_model.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Load the model (demonstration)\n",
    "loaded_model = tf.keras.models.load_model(model_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Verify loaded model works\n",
    "loaded_test_loss, loaded_test_accuracy = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nLoaded model test accuracy: {loaded_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Key Takeaways and Next Steps\n",
    "\n",
    "### What We Learned:\n",
    "1. **CNN Architecture for Text**: \n",
    "   - Embedding layer converts words to dense vectors\n",
    "   - Conv1D layers extract local n-gram features\n",
    "   - Pooling layers select most important features\n",
    "   - Multiple filter sizes capture different patterns\n",
    "\n",
    "2. **Training Best Practices**:\n",
    "   - Use early stopping to prevent overfitting\n",
    "   - Monitor validation metrics\n",
    "   - Use dropout for regularization\n",
    "   - Save best model checkpoints\n",
    "\n",
    "3. **Performance Analysis**:\n",
    "   - Evaluate on held-out test set\n",
    "   - Analyze confusion matrix\n",
    "   - Check prediction confidence distribution\n",
    "   - Examine misclassified examples\n",
    "\n",
    "### Possible Improvements:\n",
    "1. **Pre-trained Embeddings**: Use GloVe or Word2Vec\n",
    "2. **Deeper Architecture**: Add more convolutional layers\n",
    "3. **Hyperparameter Tuning**: Optimize filter sizes, number of filters, dropout rates\n",
    "4. **Data Augmentation**: Back-translation, synonym replacement\n",
    "5. **Ensemble Methods**: Combine multiple models\n",
    "6. **Attention Mechanisms**: Add attention layers to focus on important words\n",
    "\n",
    "### Resources:\n",
    "- Original Paper: \"Convolutional Neural Networks for Sentence Classification\" (Kim, 2014)\n",
    "- TensorFlow Documentation: https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
    "- Understanding CNNs for NLP: http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises for Practice:\n",
    "\n",
    "1. **Experiment with hyperparameters**: Try different embedding dimensions, filter sizes, and number of filters\n",
    "2. **Multi-class classification**: Apply this to a dataset with more than 2 classes\n",
    "3. **Custom dataset**: Use your own text data for classification\n",
    "4. **Pre-trained embeddings**: Integrate GloVe or Word2Vec embeddings\n",
    "5. **Regularization techniques**: Try L2 regularization, different dropout rates\n",
    "6. **Model interpretation**: Implement attention visualization or feature importance analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
